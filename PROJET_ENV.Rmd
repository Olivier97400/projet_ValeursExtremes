---
title: "Projet_Environnement"
author: "Guéladio NIASSE"
date: "2023-02-05"
output: html_document
---

# Introduction :

L'ojectif de ce travail est de de modéliser le comportement extrême des vagues dans le golfe du lion à l'aide de la théorie des valeurs extrêmes étudiée en cours. Elle s'étire sur 220 kilomètres de la Camargue à la frontière espagnole. Le littoral est sableux, ponctué seulement par quelques amers rocheux (Maguelone, Sète, Agde, Gruissan...). Meuble par nature, il s'est continuellement transformé ; la mer gagnant souvent sur les terres par élévation du niveau marin ou érosion côtière, mais le rivage a aussi avancé pendant de longues périodes grâce à l'apport de sédiments en provenance des fleuves. Des lagunes ont pu se former et parfois disparaitre, comme les « graus », qui font communiquer les étangs littoraux avec la mer. C'est donc un milieu naturellement dynamique. C'est dans ce contexte que nous étudions le comportement extrême des vagues de façon univariée en considérant qu'une seule station puis d'un approche bivariée en consiérant des paires de stations. Nous disposons deux jeux de données " DonneesStations" est un dataframe contenant les coordonnées géographiques des 20 stations et "DonneesVagues" est un dataframe contenant les hauteurs significatives de vagues horaires mesurées en chacune des stations précedentes de 1961 à 2012. Nous allons présenter les données dans partie suivante:

## Présentation de données:

```{r message=FALSE}
library(ismev)
library(fExtremes)
library(evd)
# library(rgl)
```

# 

```{r message=FALSE}
load("C:\\Users\\olivi\\Documents\\Montpellier\\M2_SSD\\semestre2\\Valeurs_extremes\\projet_ValeursExtremes\\donneesVagues.RData")
load("C:\\Users\\olivi\\Documents\\Montpellier\\M2_SSD\\semestre2\\Valeurs_extremes\\projet_ValeursExtremes\\donneesStations.RData")
data1 <- donneesVague
data2 <- buoysInfos
attach(data1)
#str(buoysInfos)
str(donneesVague)

summary(donneesVague)

```

Voici un petit approche de notre jeux de données sur les vagues, elle constituée de 464280 observations pour les 21 variables. La date est indiquée et correspond à la première colonne, la premiére vague a été Voici un tablau récapulatif des hauteurs mininals et maximal des différentes stations. Nous remarquons que

```{r}
substr(data1,)
```

## Cas univariée :

```{r}
# SA est de taille 464280
# On affiche les diviseurs du nombre de données dans SA
# afin de nous aider à choisir la taille n des blocs
SA = station2
li_divisor = c()
s = length(SA)
for(d in 1:s){
  if(s%%d == 0){
    li_divisor = c(li_divisor, d)
  }
}
print(li_divisor)
```

On choisi comme valeur de n: 2980 car cette valeur, nous permet d'avoir un nuage des point des max assez dispersé.

Nous pouvons choisir Calcul max par bloc

```{r}
#grp=data1[,1:3]

n=2920

SA<-station2
#SB<-station2

max_SA<-rep(0,length(SA)/n)
cat("le nombre de bloc est ",length(SA)/n,"\n")

for(i in 1:(length(SA)/n)){ max_SA[i]<-max(SA[((i-1)*n+1):(i*n)])}
maxfit<-gev.fit(max_SA)
#
plot(max_SA)

plot(SA)




```

Les points du nuages sont proches de la diagonale (ligne bleu) donc l'ajustement est de bonne qualité, le nombre de bloc retenu pour l'approche GEV est donc bien été choisi. (formule a ecrire slide 72).

```{r}
gev.diag(maxfit)

```

En observant le quantile, nous constatons pas mal du tout. Donc le choix des blocs pas mal

le montant qui sera depasser tous les 100 ans

```{r }
#p=1/100
#q=n*p
T=100
q=1/T
dep100=fgev(max_SA,prob=q)
cat(" le montant qui sera depasser tous les 100 ans est ", dep100$estimate[1])

```

les intervalle de confiances

```{r }
cat(" l'intervalle de confiance à 95%  de l'estimateur  du depassement de 100 ans  est:","\n","[" ,dep100$estimate[1]- 1.96*dep100$std.err[1],"," ,dep100$estimate[1]+ 1.96*dep100$std.err[1],"]","\n")

```

Le niveau de retour associé à la periode $T=100$ est bien comprise dans l'intervalle de confiance de niveau $1- \alpha = 95\%$. Cela nous confirme une fois que la taille des blocs choisis pour l'approche GEV est bonne.

le montant qui sera depasser tous les 500 ans

```{r }
#p=1/100
#q=n*p
T=500
q=1/T
dep500=fgev(max_SA,prob=q)
cat(" le montant qui sera depasser tous les 500 ans est ", dep500$estimate[1])

```

```{r }
cat(" l'intervalle de confiance à 95%  de l'estimateur  du depassement de 500 ans  est:","\n","[" ,dep500$estimate[1]- 1.96*dep500$std.err[1],"," ,dep500$estimate[1]+ 1.96*dep500$std.err[1],"]","\n")

```

Le niveau de retour associé à la periode $T=500$ est bien compris dans l'intervalle de confiance de niveau $1- \alpha = 95\%$. Cela nous confirme une fois que la taille des blocs choisis pour l'approche GEV est bonne.

la longueur des vague qui sera depasser tous les 1000 ans

```{r }
#p=1/100
#q=n*p
T=1000
q=1/T
dep1000=fgev(max_SA,prob=q)
cat(" le montant qui sera depasser tous les 1000 ans est ", dep1000$estimate[1])

```

les intervalle de confiance

```{r }
cat(" l'intervalle de confiance à 95%  de l'estimateur  du depassement de 100 ans  est:","\n","[" ,dep1000$estimate[1]- 1.96*dep1000$std.err[1],"," ,dep1000$estimate[1]+ 1.96*dep1000$std.err[1],"]","\n")

```

Le niveau de retour associé à la periode $T=1000$ est bien compris dans l'intervalle de confiance de niveau $1- \alpha = 95\%$. Cela nous confirme une fois que la taille des blocs choisis pour l'approche GEV est bonne.

# méthode par dépassement

cherchons le seuil

```{r }
mrlplot(SA)
```

1e5 semble etre une bonne choix

```{r }
th1=2.7
sagpd1=fpot(SA,threshold=th1)
sagpd1

```

```{r}
par(mfrow=c(2,2))
plot(sagpd1)
```

```{r}
th=3.6
sagpd=fpot(SA,threshold=th)
sagpd
```

diagnostique gravique

```{r }
par(mfrow=c(2,2))
plot(sagpd)

```

On a testé plusieurs valeur de seuil dont : 2.7, 3.8, 3.7 (Avec ces deux valeurs il y avait une erreur) et 3.6 avec laquelle on obtient la proportion d'erreur la plus petite qui vaut $0.01 \%$ contre $0.13\%$ pour la valeur de seuil égale à 2.7 On choisit donc le seuil 3.6. De même en regardant les "probability plot" et les "quantiplot plot", on voit que pour la valeur de seuil 3.6 le nuage de point colle plus a la de la diagonale (ligne bleu) que pour l'autre seuil. Ce qui nous conforte dans l'idée de choisir ce seuil (3.6).

le quantile plot n'est pas trop mal donc le choix du seuil n'est pas mal

maintenant calculons le dépassement de 100 sinistre

```{r }
#p=1/100 
#q=n*p=1/T
#p=1/nT=1/mper

T=100
q=1/T
depas100=fpot(SA,threshold=th,npp=1,mper = 100*n, std.err = FALSE)
cat(" le montant qui sera depasser tous les 100 ans est ", depas100$estimate[1],"par l'autre methode on avait ",dep100$estimate[1])

```

intervalle de confiance pour 100 sinistre

```{r }
cat(" l'intervalle de confiance à 95%  de l'estimateur  du depassement de 100 sinistre  est:","\n","[" ,depas100$estimate[1]- 1.96*dep100$std.err[1],"," ,depas100$estimate[1]+ 1.96*dep100$std.err[1],"]","\n")
```

maintenant calculons le depassement de 500

```{r }
depas500=fpot(SA,threshold=th,npp=1,mper = 500*n,std.err = FALSE)
cat(" le montant qui sera depasser tous les 500 ans est ", depas500$estimate[1],"par lautre methode on avais ",dep500$estimate[1])

```

```{r }
cat(" l'intervalle de confiance à 95%  de l'estimateur  du depassement de 100 sinistre  est:","\n","[" ,depas500$estimate[1]- 1.96*dep500$std.err[1],"," ,depas500$estimate[1]+ 1.96*dep500$std.err[1],"]","\n")
```

pour 1000 ans

```{r }
depas1000=fpot(SA,threshold=th,npp=1,mper = 1000*n,std.err = F)
cat(" le montant qui sera depasser tous les 1000 ans est  ", depas1000$estimate[1],"par l'autre methode on avais ",dep1000$estimate[1])

```

```{r }
cat(" l'intervalle de confiance à  95%  de l'estimateur  du depassement de 1000 ans  est:","\n","[" ,depas1000$estimate[1]- 1.96*depas1000$std.err[1],"," ,depas1000$estimate[1]+ 1.96*depas1000$std.err[1],"]","\n")
```

# PARIE II

Question 1. Construction des max : Construire des maxs par bloc (max par composant). Pour commencer, on pourra choisir des blocs de taille 30.

```{r}
SB = station9
li_divisor = c()
s = length(SA)
for(d in 1:s){
  if(s%%d == 0){
    li_divisor = c(li_divisor, d)
  }
}
print(li_divisor)
```

```{r}

n=2920
SA<-station2
SB=station9
SASB=cbind(SA,SB)

max_SA<-rep(0,length(SA)/n)
max_SB<-rep(0,length(SB)/n)
cat("le nombre de bloc est ",length(SA)/n,"\n")
cat("le nombre de bloc est ",length(SB)/n,"\n")

for(i in 1:(length(SA)/n)){ max_SA[i]<-max(SA[((i-1)*n+1):(i*n)])
max_SB[i]<-max(SB[((i-1)*n+1):(i*n)])}

maxSASB=data.frame(max_SA,max_SB)
par(mfrow=c(1,2))
plot(max_SB,main= "maxima de SB", col='red' )
plot(max_SA,main= "maxima de SA" , col='blue')
```

Question 2. Etude univariée : On se retrouve ainsi avec un échantillon de 96 maxima. Effectuer un ajustement par une loi GEV sur chacune des 2 marginales.

ajustement de SA

```{r}
 gev_SA=gev.fit(max_SA)
gev_SB=gev.fit(max_SB)
summary(gev_SA)
 

parametre_SA=c(gev_SA$mle[1],gev_SA$mle[2],gev_SA$mle[3])
parametre_SB=c(gev_SB$mle[1],gev_SB$mle[2],gev_SB$mle[3])

parametres<-data.frame(parametre_SA,parametre_SB)
rownames(parametres)=c("mu","sigma","gamma")
colnames(parametres)=c("parametre SA","parametre SB")
parametres


```

Question 3. Ajustement bivariée : Dans cette question, nous travaillerons directement sur les max construits à la question 1. a) Tracer le nuage de points de ces maxs.

```{r}
plot(maxSASB)

```

c)  A l'aide de la commande fbvevd, ajuster un modèle asymétrique logistique.

```{r}

mod_alog <- fbvevd(maxSASB, model = "alog")
mod_alog
par(mfrow=c(3,2))
plot(mod_alog)
```

```{r}
a=c("log", "alog", "hr")
aic= rep(0,length(a))
for(i in  a){
 print(i)
  print( fbvevd(maxSASB, model = i)$estimate)
  aic[i]=AIC(fbvevd(maxSASB, model = i))
}
aic
```

c le model logistique qui a la plus faible AIC donc c le meilleur model ajustons le

```{r}
  mod_log <- fbvevd(maxSASB, model = "log")
mod_log
par(mfrow=c(3,2))
plot(mod_log)
```

d)  Comparer les 2 modèles et discuter de la dépendance présente dans les extrémes

```{r}

nllh_model=data.frame("log"=(-AIC(mod_log)+2*3)/2,"alog"=(-AIC(mod_alog)+2*3)/2)
row.names(nllh_model)=c("a")
nllh_model
```

dapres le tableau des AIC si dessus le moins mauvais model est le model assymetrique logistique on voie que les estimation des lois marginau on quasiment les memes parametres que celle obtenue avec la methode univarier pas besoin de faire une transformation

\###################################################################

Question 4. Fonction de Pickands : Tracer les fonctions de Pickands correspondant aux modéles logistique et asymétrique logistique ainsi que celle estimée de façon non-paramétrique ? partir des max. A quoi correspond l'argument epmar de la fonction abvnonpar ?

si epmar est egale a True elle permet d'effectuer une transformation empirique des marginaux de préférence à une estimation paramètrique marginale du GEV et les nslocarguments sont ignorés.

```{r}
par(mfrow=c(1,2))
abvnonpar(data = maxSASB, plot =T, method = 'cfg',epmar = T)
abvnonpar(data = maxSASB, plot =T, method = 'cfg')



```

on observe qu'avec l'estimateur parametrique on ? pas boeucoup de dependance qu'on a pas bcp de dependance contrairement a lestimateur non parametrique on voi qu'il de la dependance

cest la on voie que le moin mauvais transformation est cel ki a la plus la grande log likhoud donc c'est la transformation marginales en gumbel

le montant qui sera depasser tous les 100 ans

```{r}
SAgumbel=qgumbel(pgev(max_SA,parametre_SA[1],parametre_SA[2],parametre_SA[3]),parametre_SA[1],parametre_SA[2])
sum(SAgumbel<4)

SB_gumbel=qgumbel(pgev(max_SB,parametre_SB[1],parametre_SB[2],parametre_SB[3]),parametre_SB[1],parametre_SB[2])
SASBgumbel=cbind(SAgumbel,SB_gumbel)

fbvevd(SASBgumbel, model = "log", std.err=F)
```

```{r }
#p=1/100
#q=n*p
T=100
q=1/T
dep100=fgev(SB_gumbel,prob=q, std.err = F)
cat(" le montant qui sera depasser tous les 100 ans est ", dep100$estimate[1])

```

les intervalle de confiance

```{r }
cat(" l'intervalle de confiance ? 95%  de l'estimateur  du depassement de 100 ans  est:","\n","[" ,dep100$estimate[1]- 1.96*dep100$std.err[1],"," ,dep100$estimate[1]+ 1.96*dep100$std.err[1],"]","\n")

```

```{r }
#p=1/100
#q=n*p
T=500
q=1/T
dep500=fgev(SB_gumbel,prob=q)
cat(" le montant qui sera depasser tous les 500 ans est ", dep500$estimate[1])

```

les intervalle de confiance

```{r }
cat(" l'intervalle de confiance ? 95%  de l'estimateur  du depassement de 500 ans  est:","\n","[" ,dep500$estimate[1]- 1.96*dep500$std.err[1],"," ,dep500$estimate[1]+ 1.96*dep500$std.err[1],"]","\n")

```

la longueur des vague qui sera depasser tous les 1000 ans

```{r }
#p=1/100
#q=n*p
T=1000
q=1/T
dep1000=fgev(SB_gumbel,prob=q)
cat(" le montant qui sera depasser tous les 1000 ans est ", dep1000$estimate[1])

```

les intervalle de confiance

```{r }
cat(" l'intervalle de confiance ? 95%  de l'estimateur  du depassement de 1000 ans  est:","\n","[" ,dep1000$estimate[1]- 1.96*dep1000$std.err[1],"," ,dep1000$estimate[1]+ 1.96*dep1000$std.err[1],"]","\n")

```

```{r}
SASBgumbel=cbind(SB_gumbel,SAgumbel)
a=qcbvnonpar(p=c(0.95),SASBgumbel,plot = T)
b=a[[1]]
plot(density(b[,2]))
plot(density(b[,1]))
```
